<imports />

<page ns="c-blog" path="blog/next-gen-memory-model">
<main>
<title>Vale's Memory Model: A Hybrid of Swift, Rust, and Arcane Sorcery</title>
<subtitle>Using generational indices as the foundation for a language!</subtitle>

<date-and-author date="August 77th, 2020" author="Someone" />


<section>
  <div className={ns("content")} style={{border: "1px solid #D0D8E0", backgroundColor: "#E0F0FF", padding: "8px"}}>(This isn't published yet, and after several waves of editing, will probably go up early September. Please dont share this page!)</div>
</section>

<section>
  <p>This page describes Vale's "next-generation" memory model.</p>
</section>

<section>
  <p>The ultimate goal of this endeavor is for Vale to have speed on par with C++, with zero unsafety, and without a restrictive borrow checker. <n>dream</n></p>
</section>


<section>
  <p>Note that this is <b>still in development</b> and <i>very</i> untested. We're sharing the idea for early feedback, and to show you where Vale is headed. We'd love your thoughts, and let us know if you see any problems or have any questions!</p>
</section>

<section>
  <p>And of course, if you want to be part of this and bring this into the world, come <a href="/contribute">come join us</a> and help make it happen!</p>
</section>


<section>
<p>
  If you already know about various sorcerous topics like free-lists, generational indices, thread-local storage, borrow checking, atomicity, fat pointers, branch misprediction, and cache misses, then feel free to skip to the last section where we have a shorter, denser explanation. Otherwise, keep reading and enjoy the ride, you're going to learn some things!
</p>
</section>

<h2>Swift, Rust, and Arcane Sorcery</h2>

<section>
<p>
  We found our inspiration from three obscure corners of the world:
</p>
<ul>
  <li>Swift's weak references decouple an object's destruction from its deallocation, a technique that can be reused to assist memory safety.</li>
  <li>In Rust, we can refer to an object with an index to an array that is paired with a "generation" number, which describes whether the object is still alive or not. We reuse this to implement zero-cost aliasing.</li>
  <li>We integrate these two things <i>into malloc() itself,</i> add a dash of static analysis, automatic borrow-checking, and low-level magic to create an entirely new approach for memory safety.</li>
</ul>

</section>

<section>
  <p>First, let's get a new perspective where Rust's speed and safety come from, and see how we can take it apart and reassemble it.</p>
</section>

<h2>What makes Rust Fast and Safe?</h2>

<section>
<p>
  One obvious answer is "the borrow checker, of course!" but the real answer is a bit more nuanced than that:
</p>
<ul>
  <li>
    The borrow checker forces people to learn and use data-oriented programming instead of object-oriented programming. <n>ecs</n>
  </li>
  <li>
    Rust programs avoid expensive heap allocations.
  </li>
  <li>
    Isolating threads' memory from each other, allowing the non-atomic <c>Rc<T></c> for reference counting. <n>shared_ptr</n>
  </li>
  <li>
    Generational indices into a central Vec. <n>vector</n>
  </li>
  <li>
    And lots of other details, of course!
  </li>
</ul>
</section>

<h3 className={ns("noline")}>Wait, what's a Generational Index?</h3>

<section>
<p>
  A "generational index" is a tactic that can be employed by any non-garbage-collected language to help with memory safety. Programs in C, C++, and even earlier languages have employed generational indices.
</p>


<h4 className={ns("noline")}>Object and Generation Tables</h4>


<p>
  In C++ terms, we would have two tables, which are always kept in sync: <n>together</n>
</p>
<ul>
  <li>An "object table" <c>std::vector<T></c> will hold all of our objects.</li>
  <li>
    A "generation table" <c>std::vector<[u32, u32]></c> <n>syntax</n> will hold all their "generation" numbers.
    <ul>
      <li>
        The first <c>u32</c> is the generation, it represents "The corresponding object in the object table is the <b>n</b>th one to inhabit that slot."
      </li>
      <li>
        The second <c>u32</c> is the "next free index", zero if the object is alive, and if the object is dead it will contain the index of the next dead object. Together, these effectively form a singly-linked list of all the free slots.
      </li>
    </ul>
  </li>
</ul>

</section>
<section>
<p>
  When we destroy an object, we increment the "actual generation" in the generation table, and add it to the list of free slots.
</p>

<p>
  As we'll see below, the generation table <b>tracks the lifetime of an object.</b>
</p>
</section>

<h4 className={ns("noline")}>GenInd: A Generation + Index</h4>

<section>

<split>
  <half>
<p>
  Just like how <c>shared_ptr<T></c> is secretly a struct which contains a T* and a pointer to a reference counter, <n>controlblock</n> we can have a <c>GenInd</c> struct containing:
</p>
<ul>
  <li>
    <c>u32 index</c> of an object in that vector.
  </li>
  <li>
    <c>u32 targetGen</c> generation of the object we're referring to.
  </li>
</ul>
<p>
  To make a <c>GenInd</c> referring to a certain object, we take that object's index and generation number (from the generation table), and copy them into the <c>GenInd</c>.
</p>
  </half>
  <half>
<pre className="cppSnippet root"><div className="cppSnippet header">C++</div><code className="cpp">
{`struct GenInd {
  uint32_t index;
  uint32_t targetGen;
}`}
</code></pre>
<br />
<pre className="cppSnippet root"><div className="cppSnippet header">C++</div><code className="cpp">
{`GenInd makeGenInd(
    const vector<[u32, u32]>& genTable,
    u32 i) {
  assert(i < genTable.size());
  // Make sure it's still alive
  assert(get<1>(genTable[i]) == 0);
  // Get the object's generation
  u32 actualGen = get<0>(genTable[i]);
  // Return a GenInd!
  return GenInd { i, actualGen };
}`}
</code></pre>
  </half>
</split>
</section>


<h4 className={ns("noline")}>Dereferencing a GenInd</h4>

<section>
<p>
  Before we "dereference" our <c>GenInd</c>, we do a <b>liveness check</b> to see whether <c>GenInd.targetGen</c> still matches the "actual generation" in the generation table.
</p>
</section>

<section>
<p>
  It's as if the reference is saying:
</p>
<p>
  <b>&nbsp;&nbsp;&nbsp;"Hello! I'm looking for the 11th inhabitant of this house, are they still around?"</b>
</p>
<p>
  and the person who opens the door says:
</p>
<p>
  <b>&nbsp;&nbsp;&nbsp;"No, sorry, I am the 12th inhabitant of this house, the 11th inhabitant is no more."</b>
</p>
<p>
  or instead:
</p>
<p>
  <b>&nbsp;&nbsp;&nbsp;"Yes! That is me. Which of my fields would you like to access?"</b>
</p>
<p>
  Now let's see some code!
</p>
</section>

<split>
  <half>
    <p>
      We won't be dereferencing our GenInd with the <c>*</c> operator like we're used to. Instead, we might have this method, which takes in the vector as an argument.
    </p>
    <p>
      This shipRef is referring to a Ship which might not be there anymore; something else may have taken its spot, in which case <c>shipRef.targetGen</c> will not match the "actual" generation at that spot in the array.
    </p>
    <p>
      If that happens, we can return a nullptr, or halt the program there if we desire.
    </p>
  </half>
  <half>
<pre className="cppSnippet root"><div className="cppSnippet header">C++++</div><code className="cpp">
{`Ship* dereference(
    const vector<[u32, u32]>& genTable,
    const vector<Ship>& ships,
    GenInd shipRef) {
  assert(shipRef.index < ships.size());
  uint64_t actualGen =
      get<0>(genTable[shipRef.index]);
  if (shipRef.targetGen != actualGen) {
    // It's gone!
    return nullptr; // or halt here
  }
  // It's still alive!
  return &ships[shipRef.index];
}
`}
</code></pre>
  </half>
</split>

<h3 className={ns("noline")}>Reference Counting</h3>

<section>
<p>
Another source of Rust programs' speed is its <c>Rc<T></c>. Whereas C++'s <c>std::shared_ptr<T></c> is slow, <c>Rc<T></c> is extremely fast, because it doesn't have to <b>atomically</b> increment and decrement the reference count integer.
</p>

<p>
For example, when <c>Rc<T></c> increments an integer (for example a <c>u32</c> named <c>x</c> containing the value 7, it does three simple things:
</p>
<ul>
  <li>Load <c>x</c>'s value from RAM into a register.</li>
  <li>Add 1 to the register (now it's 8).</li>
  <li>Store the register's value back into <c>x</c> in RAM.</li>
</ul>

</section>

<h4 className={ns("noline")}>Why doesn't C++'s shared_ptr&lt;T&gt; do that?</h4>

<section>
<p>
This works well for Rust, because all of the <c>Rc<T></c>'s for a given object are guaranteed to be in the same thread. However, C++ lets us share <c>std::shared_ptr<T></c>'s among multiple threads.
</p>
<p>
Since multiple threads can run at the same time, we might get into an awkward situation like this:
</p>
<ul>
  <li>(Thread A) Load <c>x</c>'s value from RAM into a thread A register.</li>
  <li>(Thread B) Load <c>x</c>'s value from RAM into a thread B register.</li>
  <li>(Thread B) Add 1 to the thread B register (now it's 8).</li>
  <li>(Thread A) Add 1 to the thread A register (now it's 8).</li>
  <li>(Thread A) Store thread A register's value back into <c>x</c> in RAM.</li>
  <li>(Thread B) Store thread B register's value back into <c>x</c> in RAM.</li>
</ul>
<p>
  Uh oh, they didn't see each other's changes, so even though two threads tried to increment the 7, it's only 8 when it should be 9!
</p>
</section>

<section>
<p>
  C++ uses an "atomic increment" to avoid the above problem, but it's a hundred times slower, and hinders the optimizer.
</p>
</section>

<section>
<p>
  Because Rust makes it so an object is only visible to one thread at a time, its <c>Rc<T></c>'s increments and decrements dont need to be atomic, and are way faster.
</p>
</section>

<section>
<p>
  Generational indices and non-atomic ref-counting are both amazing tools. Rust programs can also be fast because of data-oriented programming and avoiding heap allocations. We won't focus on those in this article, but check out <a href="/blog/zero-cost-refs-regions">Zero-Cost References with Regions</a> to see how Vale can avoid heap allocations.
</p>
</section>


<h3 className={ns("noline")}>Can we get this speed and safety without the borrow checker?</h3>

<section>
<p>
  Rust is an amazing language, though it's not perfect; a common lament is that <b>the borrow checker is difficult to work with.</b> <n>fight</n>
</p>
</section>

<section>
<p>
  But, as we saw, Rust's speed and safety doesn't directly come from the borrow checker: it comes from generational indices, non-atomic ref-counting, and other details. It begs the question... can we get speed and safety by just using those directly, without the borrow checker?
</p>
</section>

<section>
<p>
  <b>We believe we can!</b>
</p>
</section>

<section>
<p>
  Whereas languages like Java and Javascript integrate garbage collection into the language itself, and languages like Swift build upon atomic reference counting, <b>we can do something new: use generation numbers as the foundation for a language!</b>
</p>
</section>

<section>
<p>
On top of that, the language can employ non-atomic ref-counting and <b>automatic borrow checking</b> <n>lbt</n> to skip most liveness checks, to be even faster.
</p>
</section>



<h2>Stage 1: A Simple Beginning</h2>

<section>
<p>
  Vale's memory model is best explained in stages. <b>We'll intentionally start simple and inefficient, and then in each stage, we'll adjust the model to be more efficient.</b>
</p>
</section>

<section>
<p>
  We start with a very vanilla. simplified form of C++, where every object is owned by a <c>unique_ptr<T></c>, and we can refer to it with raw pointer <c>T*</c>s. Every primitive is simply copied around, like C++ normally does.
</p>

<p>
  For now, there is no:
</p>
<ul>
  <li>Pointer arithmetic,</li>
  <li>Pointers to members,</li>
  <li>Arrays,</li>
  <li>Inlined objects (objects in the stack or in their containing structs),</li>
  <li>Memory safety; these raw pointers can use-after-free.</li>
</ul>

<p>
  You could think of this as Python or Swift except that only one reference owns the object, and the rest are raw/unsafe/unowned pointers.
</p>
</section>

<section>
<p>
  Throughout this explanation, we'll be using the shorter <c>u64</c> to mean <c>uint64_t</c>, and use other sizes like <c>u48</c>, <c>u47</c>, <c>u15</c>, etc.
</p>

</section>


<h2>Stage 2: Adding Generation Numbers</h2>

<section>
  <p>
    First, we'll bring in the <c>std::vector<[u32, u32]></c> "generation table" from above, and use it for <b>every object in this thread.</b> This is called the <b>Local Generation Table</b>.
  </p>
  <p>
    Here, it's a <c>vector<u48></c>. Each slot can represent the lifetimes of up to 2^48 objects.
  </p>
</section>

<section>
  We cap the size of the vector at 2^47 elements, so we index into it with a u47 "LGT index". <n>47</n> <n>95</n>
</section>

<h4 className={ns("noline")}>Objects Still Live in the Heap</h4>

<section>
  <p>
    There is no central "object table" to hold our objects. Instead, they live on the heap.
  </p>

  <p>
    At the top of every heap allocation, we'll have a u47 LGT index. When we free an object, we increment the generation number at that index in the LGT.
  </p>
</section>


<h4 className={ns("noline")}>GenRef: Generation + Index + Ptr</h4>

<split>
  <width60>
<p>Similar to the C++ example's <c>GenInd</c>, our raw pointers each become a <c>GenRef</c> struct, which has: <n>159</n></p>
<ul>
  <li>
    <c>targetGen</c> The u48 generation number of the object we're referring to.
  </li>
  <li>
    <c>lgtIndex</c> The u47 index into the LGT.
  </li>
  <li>
    <c>objectPtr</c>, the raw pointer to the target object.
  </li>
</ul>
<p>
To make a GenRef referring to a certain object, we:
</p>
<ul>
  <li>Take that object's index, which was located just above it, at the top of the allocation.</li>
  <li>Use it to look up the object's generation.</li>
  <li>Copy these and objectPtr into a GenRef.</li>
</ul>
  </width60>
  <width40>
<vale>
struct GenRef<T> {
  targetGen u48;
  lgtIndex u47;
  objectPtr *T;
}

fn makeGenRef<T>(objectPtr *T)
GenRef<T> {
  // Get object's LGT index,
  // from 8b before objectPtr.
  lgtIndex = ...;
  // Get object's generation
  gen = lgt[lgtIndex.0];
  // Make the GenRef!
  ret GenRef(
    gen, lgtIndex, objectPtr);
}
</vale>
</width40>
</split>

<h4 className={ns("noline")}>Dereferencing a GenRef</h4>

<section>
  Dereferencing works in a way similar to C++ example.
</section>


<split>
  <half>
<p>
To dereference a GenRef, we:
</p>
<ul>
  <li>Take that object's index, which was located just above it, at the top of the allocation.</li>
  <li>Use it to look up the object's actual generation.</li>
  <li>Assert the GenRef's targetGen equals the object's actual generation.</li>
  <li>Return the GenRef's objectPtr.</li>
</ul>
  </half>
  <half>
<vale>
// Using above GenRef<T>

fn deref<T>(genRef GenRef<T>) *T {
  targetGen = genRef.targetGen;
  lgtIndex = genRef.lgtIndex;

  actualGen = lgt[lgtIndex].0;
  if (targetGen == actualGen) { «345»
    panic();
  }

  // It's still alive!
  ret genRef.objectPtr;
}
</vale>
</half>
</split>

<h4 className={ns("noline")}>The Picture So Far</h4>

<section>
  <p>
    So far, this is a similar system as the C++ example before, but with a big difference: we use it for all objects, and objects live on the heap instead of in an array.
  </p>
  <p>
    We still have the Local Generation Table, though each entry is a [u48, u47] rather than a [u32, u32]. The u47, which serves as the index of the next free object, is reduced to a u16 in a later stage.
  </p>
</section>

<section>

</section>





<h2>Next Steps</h2>

<section>
<p>In coming stages, we'll:</p>

<ul>
  <li>Make it so we can have objects on the stack, and inline inside a containing struct.</li>
  <li>Add a mechanism to automatically "lock" an allocation to skip liveness checks for given scopes.</li>
  <li>Skip liveness checks by using static analysis to trace references through intermediate structs.</li>
  <li>Eliminate the cache-miss for the vast majority of liveness checks.</li>
  <li>Shrink GenRef down to 128 bits.</li>
  <li>Make it so the GenRef liveness check doesn't involve branching.</li>
  <li>Make functions able to communicate requirements about parameters, to skip even more liveness checks.</li>
</ul>
<p>
  <b>Stay tuned for coming articles which will describe those stages in detail!</b> <n>tuned</n>
</p>
</section>

<section>
  <p>
    Vale is still a work in progress, and we're implementing these stages right now. Generations have never been used this way, and we're discovering new potential every day.
  </p>
</section>

<section>
  <p>
    If you want to see this happen sooner, or just want to contribute to something cool, we invite you to <a href="/contribute">come join us!</a> <n>help</n>
  </p>
  <p>
    We'd love to hear your thoughts on using generation numbers as the foundation of a memory model, so <a href="#">leave a comment</a>!
  </p>
</section>


<div className={ns("afterword")}>
  <h2 className={ns("noline")} style={{marginTop: 0}}>Afterword: An Arcane Telling of What's to Come</h2>

<section>
<p>
  We tip our hats to those of you who brave this section! This is a very short and dense yet complete explanation of the entire final memory model, for those who are too excited to wait for more us to craft more detailed, readable explanations in future articles.
</p>
</section>
<section>
<p>
  This assumes you're already familiar with free-lists, generational indices, thread-local storage, borrow checking, atomicity, fat pointers, branch misprediction, and cache misses.
</p>
<p>
  If you have any questions, feel free to come by the <a href="https://discord.gg/SNB8yGH">Vale discord server</a> and tell us where we can make the explanation clearer!
</p>
</section>
<section>
<p>
  In this explanation, the model starts out slow, and gets progressively more efficient.
</p>
</section>

<section>
  <ol>
    <li>
      First, start with a simplified C++, where every object is owned by an owning reference (like unique_ptr), and we can refer to them with raw pointers.
      <ul>
        <li>
          Also isolate each thread's memory like Rust does; only one thread can have a reference to an object at any given time.
        </li>
      </ul>
    </li>
    <li>
      Introduce "generation numbers" and the Local Generation Table:
      <ul>
        <li>
          In every allocation (whether stack or heap), before the object, is a 47b index into a thread-global vector&lt;[u48, u16]&gt; called the Local Generation Table (LGT).
        </li>
        <li>
          Deallocating the object will increment the first u48 (the "generation number") and use the u16 to add this "open space" to a free-list.
        </li>
        <li>
          Instead of a raw pointer, every reference is a "fat pointer" <c>GenRef</c> struct containing the raw pointer, the same 47b index as the object, and a copy of the object's 48b generation number. (This 159b shrinks to 128b later)
        </li>
        <li>
          "Liveness check": If a GenRef's generation number copy matches the one in the table at that index, the object is still alive and safe to dereference.
        </li>
        <li>
          We do a liveness check every time we dereference an object.
        </li>
      </ul>
    </li>
    <li>
      We can actually already "inline" structs into the stack and containing structs; when we make a GenRef to an inlined struct, just use the containing <b>allocation</b>'s u47 LGT index.
    </li>
    <li>
      We'd like to avoid the liveness check as much as possible, and we aren't using the LGT entry's second u16 for anything while the object is alive, so we'll make that into a nonatomic <b>"delay" ref-count</b> (like Swift's weak-ref-count), which the compiler can use to delay deallocating the object for a given scope.
      <ul>
        <li>When the compiler detects that we dereference <n>derefmiss</n> a certain reference multiple times, the compiler will instead do a liveness check, increment the ref-count, and then decrement it again at the end of the block.</li>
        <li>During that scope, we can dereference the object and any of its indirectly <b>owned</b> objects freely without liveness checks.</li>
        <li>The object's owning reference counts towards that ref-count.</li>
        <li>When we decrement that count, we check if it hit zero, and if so, deallocate the object.</li>
      </ul>
    </li>
    <li>
      We'd like to avoid even more liveness checks. If we make a new struct and assign a <b>delayed</b> reference into one of its const fields and read that later, all within the scope of the delay, we can skip the liveness check for that as well.
    </li>
    <li>
      Every liveness check incurs a cache miss as it accesses the LGT. We can make it so at least heap objects don't incur the extra cache miss by changing the heap allocation's u47 to not be an index into the LGT but instead be <b>the u48 generation number itself.</b>
      <ul>
        <li>
          Compiler never calls free(), which would risk a subsequent malloc() caller overwriting the allocation's generation number. It instead calls genFree() which adds the allocation to a free-list. It has a separate heap per size class.
        </li>
        <li>
          Compiler calls genMalloc which pulls from genFree's free list, or if none, calls malloc().
        </li>
        <li>
          Every reference needs to know whether it's pointing at a stack object or a heap object. We'll add 1 "isHeap" bit to GenRef (160b now).
        </li>
        <li>
          If isHeap is false, treat the remaining 159b as we did before, use it to look up the object's generation in the LGT.
        </li>
        <li>
          If isHeap is true, the GenRef has three things...
          <ul>
            <li>Pointer to object, as before.</li>
            <li>u48 generation, as before.</li>
            <li>Pointer to the u48 generation number at top of the allocation.</li>
          </ul>
          ...for a total of 177b (shrunk to 128b later).
        </li>
      </ul>
      Objects on the stack still access the LGT. <n>stacklgt</n>
    </li>
    <li>
      The GenRef is now 177b, and we'd like to shrink it to 128b, for more compact data. We'll still have the 1b isHeap.
      <ul>
        <li>
          If isHeap is false, the GenRef contains:
          <ul>
            <li>
              u47 LGT index, as before.
            </li>
            <li>
              u48 generation number, as before.
            </li>
            <li>
              <s>64b pointer</s> u32 offset, relative to the start of this thread's stack.
            </li>
          </ul>
        </li>
        <li>
          If isHeap is true, the GenRef contains:
          <ul>
            <li>
              48b generation number, as before.
            </li>
            <li>
              64b pointer to object, as before.
            </li>
            <li>
              <s>64b pointer</s> 15b offset, relative to above pointer, to the generation number at allocation's start.
            </li>
          </ul>
        </li>
      </ul>
      The GenRef is now 128b total.
    </li>
    <li>
      We'd like to avoid branching.
      <ul>
        <li>
          Currently, the liveness check involves some branching to figure out where the generation number is, which risks mispredictions. We can avoid that by calculating the pointer from the GenRef <i>as if</i> isHeap was true, and as if isHeap was false, multiply by isHeap and !isHeap respectively, and add the two together.
        </li>
        <li>
          Instead of using an if-statement whose condition is a boolean from comparing the target generation to the actual generation, we can multiply the objectPtr pointer by that boolean. If the generations didn't match, the boolean will be false, producing a null pointer. Dereferencing that null pointer will crash the program, mission accomplished.
        </li>
      </ul>
    </li>
    <li>
      Non-virtual functions can require that the caller lock an object before handing it in; the callee can then skip liveness checks for that reference within the function.
    </li>
  </ol>
</section>

<section>
  <p>
    Congratulations for reaching the end!
  </p>

  <p>
    Some extra details:
  </p>
  <ul>
    <li>
      GenRef is 128b which may seem large, but this is the width of C++'s <c>std::shared_ptr<T></c> and <c>std::weak_ptr<T></c>, and the default size of generational_arena's Index.
    </li>
    <li>
      genMalloc and genFree will likely be a modification of an existing malloc library like mimalloc. It has roughly the same requirements plus never overwriting that generation number at the top of every allocation.
    </li>
    <li>
      When we move something across thread boundaries, we must recurse through and increment all the generation numbers, effectively cutting off access to the rest of this thread.
    </li>
    <li>
      Anything that can travel across thread boundaries must be annotated with the <c>mig</c>ratory keyword, so we do atomic ref-counting for immutables, and don't use the ref-counting optimization for it unless we can prove it doesn't travel to another thread in a certain scope. <c>mig</c> is applied deeply.
    </li>
    <li>
      The LGT's free-list also has a touch of sorcery. The LGT is divided into 2^16-entry chunks. The u16 in each entry describes where in this chunk the next open space is. The first entry in this chunk is special; its u48 contains not a generation number but instead the index of the next chunk that's completely full of open spaces. In this way, the LGT has a free-list of free-lists.
    </li>
    <li>
      In the [u48, u16], 48 and 16 bits were chosen arbitrarily.
    </li>
    <li>
      When a generation number hits the maximum, don't use that generation number anymore; the LGT shouldnt use it, and genFree shouldn't let it be reused. genFree can slice up the allocation into smaller ones that don't include the initial 8b.
    </li>
  </ul>
</section>

<section>
  <p>
    Some potential weaknesses:
  </p>
  <ul>
    <li>
      Limits:
      <ul>
        <li>Individual structs cannot be larger than 32kb.</li>
        <li>Limited to 2^95 stack allocations over the lifetime of the thread (a 5ghz processor would take about 268 million years to hit this limit).</li>
        <li>Note: we could increase the struct size if we lower the stack allocations limit. Seems wise.</li>
      </ul>
    </li>
    <li>
      The pointer magic might prevent some LLVM optimizations, in which case we might have to augment LLVM itself or write our own optimization passes.
    </li>
    <li>
      The liveness check involves several arithmetic operations and a few data dependencies. Arithmetic is basically free, but the data dependencies could stall the CPU.
    </li>
    <li>
      Storing the generation number at the top of &lt;=64b allocation means a liveness check won't incur an extra cache miss since we're about to dereference the object anyway, and the entire object is on one cache line. However, for larger objects, it does incur an extra cache miss. Most objects are small, but programs with an unusually large proportion of medium sized objects not in an array could suffer a small performance hit.
    </li>
    <li>
      Having a separate heap per size class could increase fragmentation. This might be mitigating by using the regular malloc() and free() for large structs.
    </li>
  </ul>
</section>

</div>

</main>

<margin>

<Note name="shared_ptr">
  Rust's <c>Rc<RefCell<T>></c> is like C++'s <c>std::shared_ptr<T></c>.
</Note>

{/*<Note name="split">
  We can sometimes work around this with Vec's <c>split_at_mut</c> method to get around this, but it's not always possible, and adds its own run-time costs.
</Note>*/}

<Note name="vector">
  Rust's <c>Vec<T></c> is like C++'s <c>std::vector<T></c>.
</Note>

<Note name="derefmiss">
  Normally, ref-counting is expensive because it incurs an extra cache miss. Here, it's not extra, because it's in the object we're about to dereference anyway.
</Note>

<Note name="controlblock">
  More precisely, a <c>shared_ptr<T></c> is actually a a struct that contains a <c>T*</c> and a pointer to a "control block" struct which contains a "strong count", a "weak count", and another <c>T*</c>.
</Note>

<Note name="ecs">
  Some might say that not being able to do object-oriented programming is a weakness and makes it a poor choice for some situations. However, we instead think that this was a wise tradeoff for Rust's intended use cases (low level systems, data processing, etc).
</Note>

<Note name="fight">
  Working with the borrow checker isn't always difficult; one eventually learns the toolbox of tricks and design patterns to work around the borrow checker, and after a while you can know which architectures are compatible with it, at which point it "clicks" and you know how to avoid these battles with the borrow checker.
</Note>

<Note name="together">
  Most implementations might actually combine these to be a <c>std::vector<[u32, u32, T]></c>. Having separate lists makes it easier to understand some concepts further below.
</Note>

<Note name="47">
  It's coincidence that 47 is one away from 48; these numbers are unrelated.
</Note>

<Note name="95">
  Over the lifetime of the thread, we can have 2^95 total allocations. This limit is addressed in a later stage.
</Note>

<Note name="syntax">
  Throughout this article, we'll use shorthand such as <c>u32</c> instead of <c>uint32_t</c>, and <c>[u32, u32]</c> instead of <c>std::tuple<u32, u32></c>, sometimes even in C++ code.
</Note>

<Note name="lbt">
  Not to be confused with the region borrow checker, described in <a href="/blog/zero-cost-refs-regions">Zero-Cost References with Regions</a>.
</Note>

<Note name="159">
  This struct is 159 bytes, but a later stage will shrink this down to 128, the same size as <a href="https://github.com/fitzgen/generational-arena">generational_arena::Index</a> and <c>std::shared_ptr<T></c>.
</Note>

<Note name="345">
  This if-statement can be expensive, but it's reduced to a couple non-branching instructions in a later stage.
</Note>

<Note name="dream">
  A C++ developer's dream!
</Note>

<Note name="tuned">
  The <a href="https://reddit.com/r/vale">Vale subreddit</a> is the best place to watch for the next articles. We'll also be sharing early drafts in the <a href="https://discord.gg/SNB8yGH">Vale discord server</a>!
</Note>

<Note name="help">
  <p>All contributions are welcome! Soon, we're going to:</p>
  <ul>
    <li>Implement the remaining stages of the memory model!</li>
    <li>Finish designing the region borrow checker!</li>
    <li>Implement the bump allocator and pooling!</li>
    <li>Write a standard library! (sets, hash maps, lists, etc)</li>
    <li>Make syntax highlighters! (VSCode, Sublime, Vim, Emacs, etc)</li>
    <li>Enable support gdb/lldb for debugging!</li>
    <li>Add better error reporting!</li>
    <li>Replace the temporary combinator-based parser with a real one!</li>
    <li>Add a "show all constraint refs" option in debug mode to our LLVM codegen stage!</li>
  </ul>
  <p>If any of this interests you, come join us!</p>
</Note>

<Note name="stacklgt">
  <p>
    Since they're stack objects, there will be a limited number of them and they will be accessing it much more frequently, which means each spot will likely be hot in the cache.
  </p>
  <p>
    Additionally, for the vast majority of programs, these slots in the LGT will be accessed in a very stack-like pattern, which should be very cache-friendly.
  </p>
</Note>

</margin>
</page>
